
    <!DOCTYPE html>
    <html>
    <head>
      <title>viveknathani - designing data intensive applications</title>
      <meta name="viewport" content="width=device-width, initial-scale=1">
      <meta charset="utf-8">
      <link rel="stylesheet" type="text/css" href="/theme.css">
      <meta property="og:title" content="designing data intensive applications">
      <meta property="og:description" content="viveknathani - notes">
    </head>

    <!-- Google tag (gtag.js) -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-NJ89W10549"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());

      gtag('config', 'G-NJ89W10549');
    </script>

    <body>
      <main>
        
        <h1 id="designingdataintensiveapplications">designing data intensive applications</h1>
<ol>
<li>goal is to make systems that are reliable, scalable, and maintainable.</li>
<li>data models: relational, document, graph, triple store.</li>
<li>appending to a file is generally very efficient, most databases internally use a log, which is an append-only data file.</li>
<li>an index is an additional data structure that is derived from your primary data.</li>
<li>a well-chosen index can speed up your read queries, but every index will slow down your writes.</li>
<li>simplest index is the hash index. for a DB that stores key-value pairs on disk, a hash index can be an in-memory map for every key to its corresponding byte offset in the data file. only caveat here is that the entire index stays in memory so it has limitations w.r.t. the available RAM. apparently, this is also what bitcask does.</li>
<li>TODO: read the bitcask paper and implement a mini version of it.</li>
<li>if you just keep appending to a file, you will eventually run out of space on the disk. solution: break the log into segments of a certain size. when the size has reached, start writing in a new file. later, perform compaction on all these segments. throw away duplicate keys in the log. only keep the most recent update for each key. after this, size of the segments will reduce. merge these segments. compaction and merging can happen in background. each segment gets it's own in-memory hash table.</li>
<li>binary format is typically the best for logs.</li>
<li>also, the term "log" is used here in a more general sense to represent records of data than just limiting it to the idea of application logs.</li>
<li>deleting a record from this DB requires a special deletion record to be added which is sometimes called as the tombstone. key will get discarded during the merging process.</li>
<li>checksums can prevent having partially written records - see how.</li>
<li>control concurrency here by having a single writer thread.</li>
<li>range queries are a mess in this structure - you will have to lookup all the keys in a given range in your hash table.</li>
<li>new idea - store keys in sorted order in the segment file. makes merging process efficient - use mergesort.</li>
<li>you no longer need to keep the index of all the keys in memory - you will have a sparse index.</li>
<li>the file that stores the sorted key-value pairs is called the SSTable (sorted string table).</li>
<li>maintaining a sorted structure on disk is possible but it is easier to maintain it in memory using a red black tree or an AVL tree. when the size of the tree reaches a threshold, write it out to disk.</li>
<li>the in memory balanced tree is often called as the memtable.</li>
<li>combination of the memtable + SSTable is called as the log structured merge tree. they are an efficient altenrative to B+ trees as they scale writes better.</li>
<li>gkcs made a good video on LSM.</li>
<li>another form of index is the b-tree. b-trees keep key-value pairs sorted by key.</li>
<li>b-trees break the database down into fixed-size blocks or pages and read or write one page at a time.</li>
<li>since b-trees have to overwrite a page on disk with new data, it can happen that the data gets corrupted in event of a database crash. to make it resilient, b-trees uses an additional structure on disk - the write ahead log.</li>
<li>concurrency in writing to b-trees is solved by using latches (lightweight locks).</li>
<li>TODO: implement a b-tree and read about it's optimizations.</li>
<li>compare b-trees with LSM trees.</li>
<li>you can have your own indexes in a table - the secondary indexes.</li>
<li>seondary indexes can be clustered (store the entire row as the value)  or non clustered (store reference to the row as value) or covering (store only a few columns).</li>
<li>multi-column indexes have keys that represent more than one column.</li>
<li>you can also have a fuzzy index to support full-text search.</li>
<li>use OLAP instead of OLTP for analytical queries at scale. data warehouse over a database.</li>
<li>the interface language to a data warehouse is mostly SQL. and the data model of a data warehouse is mostly relational.</li>
<li>to speed up queries in OLAP - the storage of data can be column-oriented instead of row-oriented.</li>
<li>an idea that keeps coming up frequently is that you can compress blocks/pages/columns of data to save disk space.</li>
<li>chapter-3 needs to be read multiple times in order to fully internalise what is truly going on. great book!</li>
<li>replication - keep a copy of the same data on multiple machines. why? be geographically closer to your users, allow system to be available if some parts have failed, scale out the number of machines that can serve read queries.</li>
<li>popular algorithms: single-leader, multi-leader, leaderless.</li>
<li>in a leader based approach - the leader is the one where writes happen. and other "follower" nodes just get their copies updated.</li>
<li>replication can be: synchronous, asynchronous, semi-synchronous (copy to one node in sync and to rest async).</li>
<li>mostly, replication is fast. but under heavy workloads, it can slow down. seen this at Investmint a couple of times.</li>
<li>when you want to setup a new follower, take a snapshot of the existing data and capture the timestamp. copy the data to your new node. next, use the timestamp to get all updates after this in the database log. in postgres, instead of timestamp, log sequence numbers are used. in mysql, they are called as binlog coordinates.</li>
<li>when a node goes down: if it is a follower, it just needs to catch-up with the new updates whenever it is back. when it is a leader who has failed, it may require manual intervention. a new leader needs to be chosen and the system needs to be configured to use the new leader. it is possible that the new leader might be a little out of sync with the leader who died. this can lead to discarding of some data. this can be undesirable for some systems. github ran into this problem because they had auto incrementing ids in their system and the sequencing got messed up because the leader went down and the best possible was a little out of sync. in some situations, two nodes might believe that they are the leader. this is called a split brain situation. in general, choosing a new leader is usually ver hard. some companies prefer to just do this manually.</li>
<li>replication log implementation: statement, WAL shipping, logical row based, trigger based</li>
<li>workarounds for replication lag: have a read-after-write consistency guarantee, do monotonic reads, do consistent prefix reads.</li>
<li>multi leader replication is hard and rarely needed. one usecase where it is required: having multiple data centers in different geographies. you need to think about stuff like conflict resolution (which has a lot in parallel with the problem of implementing realtime collaborative editing). clients that support offline operations in multiple devices have a similar multi leader pattern. checkout couchdb.</li>
<li>ways for resolving write conflict: last write wins, or show merged values, or let the application resolve it instead of doing it in the DB layer (bucardo and couchdb let you do this).</li>
<li>areas of research in conflict resolution: conflict free replicated datatypes (CRDTs), meregable persistent data structures, operational transformation (used behind etherpad and google docs).</li>
<li>leaderless replication - the idea existed in the earlier days but became fashionable after amazon adopted it. writing to all nodes happens in parallel. reading from all happens in parallel too. once items are fetched, the one with the latest version number is considered to be valid.</li>
<li>strategies for fixing out-of-sync nodes: read repair, anti entropy.</li>
<li>if there are n replicas, every write must be confirmed by w nodes to be considered successful, and we must query at least r nodes for each read. as long as w + r &gt; n, we can expect to get an up-to-date value. reads and writes that obey these r and w values are called quorum reads and writes. n, w, and r are typically configurable. if fewer than w or r nodes are available, writes or reads return an error.</li>
<li>limitations of quorum consistency - sloppy quorums, concurrent writes, concurrent read and write, no rollback of values in case of partial failure.</li>
<li>dynamo-style databases are handled for cases that can tolerate eventual consistency.</li>
<li>monitoring is harder in leaderless replication.</li>
<li>fix sloppy quorums by "hinted handoff". although, sloppy quorums increase write availability.</li>
<li>leaderless replication is helpful in multi datacenter operations.</li>
<li>look into the algorithm to determine if two operations are concurrent or whether one happened before the other.</li>
<li>version vectors help in concurrent writes too.</li>
<li>sharding: watch the gkcs video.</li>
<li>transactions: group several reads and writes into one logical unit. either you commit, or rollback.</li>
<li>they have a cost: can decrease performance slightly, make systems less available.</li>
<li>the NoSQL guys dislike it, call it the "antithesis of scalability".</li>
<li>the safety guarantees of transactions are defined by ACID - atomicity, consistency, isolation, durability.</li>
<li>atomicity: all or nothing behaviour.</li>
<li>consistency: in context of ACID, it means keep the system in a "good state". however, the invariants of a system depend on the application, not on the database. so consistency in ACID is just there, without much importance.</li>
<li>isolation: two transactions are unaware about each other's existence.</li>
<li>durability: a promise that once a transaction has been commited, any data it has written will not be forgotten. however, nothing in perfect. disks can crash or get corrupt at a later point in time. unless you have backups, nothing can save you.</li>
<li>TODO: explore storage systems.</li>
<li>serializable isolation - the database guarantees that transactions have the same effect as if they ran serially.</li>
<li>however, isolation has a performance cost. so many systems prefer to provide weaker levels of isolation.</li>
<li>approach one - read committed. no dirty reads (read only what has been committed). no dirty writes (overwrrite only what has been committed).</li>
<li>issue with read committed - read skew or nonrepeatable read.</li>
<li>solution to long running read queries like backups or analytical tasks - snapshot isolation.</li>
<li>the database may need to maintain several different versions of the same object at the same time for different transactions involving that object - this is called multi version concurrency control.</li>
<li>indexing on snapshot isolation is wild. explore this.</li>
<li>postgres calls its snapshot isolation implementation as a repeatable read.</li>
<li>to prevent lost updates - atomic writes, explicit locks, automatically detect lost updates, compare-and-set.</li>
<li>explore phantoms and write skews.</li>
<li>explore serializability.</li>
</ol>
<p>troubles in distributed systems</p>
<ol>
<li>partial failures</li>
<li>unreliable networks<ol>
<li>timeouts</li>
<li>unbounded delays</li>
<li>congestion</li>
<li>queuing</li></ol></li>
<li>variable delays are a consequence of dynamic resource partitioning</li>
<li>unreliable clocks<ol>
<li>a day may not have exactly 86400 seconds</li>
<li>time-of-day clocks might move backwards in time</li>
<li>monotonic clocks can helpful in measuring duration </li>
<li>time-of-day clocks are usually synchronised with NTP which can be inaccurate because of network round trip and quartz drift.</li>
<li>logical clocks are hence looked at as the alternative. just increment counters. and use this to understand the relative ordering of events. eg: lamport clock.</li></ol></li>
</ol>
<p>consistency and consensus</p>
<ol>
<li>linearizability<ol>
<li>make the system appear as if there is only a single copy of data. a read followed by a write will receive the most up to date version of the data.</li>
<li>if any one read has returned the new value, all following reads must also return the new value. the value should never flip back to the old one.</li>
<li>you can theoretically check if a system’s behaviour is linearisable by recording the timings of all requests and responses and arranging them in a valid sequential order.</li>
<li>single leader replication has the potential to provide linearizability.</li>
<li>some consensus algorithms can guarantee linearizability. zookeeper and etcd are examples of systems that use these algorithms.</li>
<li>if your application requires linearizability and some replicas are down, then it can cause unavailability. </li>
<li>Attiya and Welch prove that if you want linearizability, the response time of read and write requests is atleast proportional to the uncertainty of delays in the network.</li></ol></li>
<li>CAP theorem<ol>
<li>states that you can have a system is either consistent and partition tolerant (CP) OR available and partition tolerant (AP) OR consistent and available.</li>
<li>however, this theorem has many misleading definitions and assumptions and only serves as a high level thinking ground. it is of little practical value in designing systems.</li></ol></li>
<li>ordering<ol>
<li>ordering of events helps in preserving causality </li>
<li>causal order is not total order</li>
<li>a linearizable system has total order</li>
<li>you can order events in a sequence using lamport timestamps</li>
<li>total order broadcast ensures that all nodes in the distributed system agree on the order in which messages are delivered, regardless of the sender. this property is crucial for maintaining consistency and ensuring that all participants have a consistent view of the system state.</li>
<li>a linearizable compare-and-set register and total order broadcast are both equivalent to having consensus.</li></ol></li>
<li>consensus<ol>
<li>2PC - coordinator sends “prepare”, sends “commit” - has single point of failure if coordinator is down.</li>
<li>3PC solves for SPOF but it assumes a network with bounded delay and nodes with bounded response times - both of which are pretty unrealistic.</li>
<li>good consensus algorithms don’t let you reach this dead state - they always move forward.</li>
<li>popular ones: viewstamped replication, paxos, raft, and zab. </li>
<li>they all implement total order broadcast.</li>
<li>leader is elected by votes.</li>
<li>most consensus algorithms assume there to be a fixed set of nodes. they rely on timeouts to detect failed nodes. they are sensitive to unreliable networks. </li>
<li>explore zookeeper and etcd - see how they are used. </li></ol></li>
</ol>
        <p></p>
        <a href="/"><- back to home</a>
      </main>
    </body>
    </html>
  