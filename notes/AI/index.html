
    <!DOCTYPE html>
    <html>
    <head>
      <title>viveknathani - AI</title>
      <meta name="viewport" content="width=device-width, initial-scale=1">
      <meta charset="utf-8">
      <link rel="stylesheet" type="text/css" href="/theme.css">
      <meta property="og:title" content="AI">
      <meta property="og:description" content="viveknathani - notes">
    </head>

    <!-- Google tag (gtag.js) -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-NJ89W10549"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());

      gtag('config', 'G-NJ89W10549');
    </script>

    <body>
      <main>
        
        <h1 id="ai">AI</h1>
<ol>
<li>what is ML? - study of algorithms that learn from data and work well on unseen data without explicit instructions being provided.</li>
<li>categories of machine learning algorithms - supervised learning, unsupervised learning, recommender systems, reinforcement learning.</li>
<li>supervised learning - algorithms that learn input-to-output mappings (X -&gt; Y). example: spam filtering, speech recognition, machine translation, online advertising.</li>
<li>regression - predict a number</li>
<li>classification - predict categories</li>
<li>unsupervised learning - find patterns in data. example: form clusters like in google news.</li>
<li>another form of unsupervised learning is anomaly detection - find unusual data points.</li>
<li>yet another form of unsupervised learning is dimensionality reduction - compress data using fewer numbers.</li>
<li>linear regression: f(x) = wx + b. w and b are parameters/coeffeicients/weights. f is the hypothesis function.</li>
<li>goal is to find w and b such that y^(i) is close to y(i) for all (x(i), y(i)).</li>
<li>cost function = (sum from i=1 to i=m [(y^(i) - y(i)) * (y^ - y(i))])/2 * m = (sum of square of errors)/2m.</li>
<li>the extra division by 2 makes future calculations easier. but the cost function can fundamentally work without it as well.</li>
<li>people can come up with their own cost functions. squared error cost function just happens to be the most used one for regression problems.</li>
<li>goal is to minimise the output of the cost function. contour plots are a good way to visualise this.</li>
<li>silver bullet for this: gradient descent.</li>
<li>w = w - alpha * (partial derivative of cost function w.r.t. w). b = b - alpha * (partial derivative of cost function w.r.t. b)</li>
<li>alpha = learning rate. small value means more iterations hence slow. large value can cause gradient descent to never reach the minimum.</li>
<li>w and b need to be updated simultaneously.</li>
<li>gradient descent can fundamentally cause you to end up at a local minima but in the case of linear regression, the cost function is a convex function - it will always converge to the global minimum.</li>
<li>TODO: implement linear regression from scratch in python.</li>
</ol>
        <p></p>
        <a href="/"><- back to home</a>
      </main>
    </body>
    </html>
  